server:
  port: 8282

spring:
  application:
    name: chat-service
  ai:
    ollama:
      chat:
        model: llama2
      embedding:
        model: llama2
  threads:
    virtual:
      enabled: true

logging:
  pattern:
    correlation: '[%X{traceId:-}-%X{spanId:-}] [%X{tenantId:-}] '

management:
  endpoints:
    web:
      exposure:
        include: "*"
  metrics:
    tags:
      application: ${spring.application.name}
    distribution:
      percentiles-histogram:
        all: true
        http.server.requests: true
  opentelemetry:
    resource-attributes:
      application: ${spring.application.name}
      "service.name": ${spring.application.name}
  otlp:
    tracing:
      endpoint: http://localhost:4318/v1/traces
  tracing:
    sampling:
      probability: 1.0
  prometheus:
    metrics:
      export:
        step: 5s
  endpoint:
    health:
      probes:
        enabled: true
      show-details: always
      show-components: always

multitenancy:
  tenants:
    - identifier: dukes
      enabled: true
    - identifier: beans
      enabled: true
    - identifier: trixie
      enabled: false
